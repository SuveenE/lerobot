# Copyright 2025 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from collections.abc import Callable
from dataclasses import dataclass, field
from pathlib import Path

import torch

from lerobot.robots.config import RobotConfig
from lerobot.teleoperators.config import TeleoperatorConfig

from .constants import (
    DEFAULT_FPS,
    DEFAULT_INFERENCE_LATENCY,
    DEFAULT_OBS_QUEUE_TIMEOUT,
)

# Aggregate function registry for CLI usage
AGGREGATE_FUNCTIONS = {
    "weighted_average": lambda old, new: 0.3 * old + 0.7 * new,
    "latest_only": lambda old, new: new,
    "average": lambda old, new: 0.5 * old + 0.5 * new,
    "conservative": lambda old, new: 0.7 * old + 0.3 * new,
}


def get_aggregate_function(name: str) -> Callable[[torch.Tensor, torch.Tensor], torch.Tensor]:
    """Get aggregate function by name from registry."""
    if name not in AGGREGATE_FUNCTIONS:
        available = list(AGGREGATE_FUNCTIONS.keys())
        raise ValueError(f"Unknown aggregate function '{name}'. Available: {available}")
    return AGGREGATE_FUNCTIONS[name]


@dataclass
class PolicyServerConfig:
    """Configuration for PolicyServer.

    This class defines all configurable parameters for the PolicyServer,
    including networking settings and action chunking specifications.
    """

    # Networking configuration
    host: str = field(default="localhost", metadata={"help": "Host address to bind the server to"})
    port: int = field(default=8080, metadata={"help": "Port number to bind the server to"})

    # Timing configuration
    fps: int = field(default=DEFAULT_FPS, metadata={"help": "Frames per second"})
    inference_latency: float = field(
        default=DEFAULT_INFERENCE_LATENCY, metadata={"help": "Target inference latency in seconds"}
    )

    obs_queue_timeout: float = field(
        default=DEFAULT_OBS_QUEUE_TIMEOUT, metadata={"help": "Timeout for observation queue in seconds"}
    )

    def __post_init__(self):
        """Validate configuration after initialization."""
        if self.port < 1 or self.port > 65535:
            raise ValueError(f"Port must be between 1 and 65535, got {self.port}")

        if self.environment_dt <= 0:
            raise ValueError(f"environment_dt must be positive, got {self.environment_dt}")

        if self.inference_latency < 0:
            raise ValueError(f"inference_latency must be non-negative, got {self.inference_latency}")

        if self.obs_queue_timeout < 0:
            raise ValueError(f"obs_queue_timeout must be non-negative, got {self.obs_queue_timeout}")

    @classmethod
    def from_dict(cls, config_dict: dict) -> "PolicyServerConfig":
        """Create a PolicyServerConfig from a dictionary."""
        return cls(**config_dict)

    @property
    def environment_dt(self) -> float:
        """Environment time step, in seconds"""
        return 1 / self.fps

    def to_dict(self) -> dict:
        """Convert the configuration to a dictionary."""
        return {
            "host": self.host,
            "port": self.port,
            "fps": self.fps,
            "environment_dt": self.environment_dt,
            "inference_latency": self.inference_latency,
        }


@dataclass
class DatasetRecordingConfig:
    """Configuration for dataset recording during evaluation.

    This class defines all configurable parameters for recording evaluation
    rollouts as a LeRobotDataset v3.
    """

    # Enable/disable recording
    enabled: bool = field(default=False, metadata={"help": "Enable dataset recording during evaluation"})

    # Dataset identifier (e.g., 'username/eval_yam_async')
    repo_id: str | None = field(default=None, metadata={"help": "Dataset repository ID (e.g., 'user/eval_dataset')"})

    # Local directory override
    root: str | Path | None = field(default=None, metadata={"help": "Local directory to store the dataset"})

    # Push to HuggingFace Hub on completion
    push_to_hub: bool = field(default=False, metadata={"help": "Push dataset to HuggingFace Hub on completion"})

    # Make the dataset private on the Hub
    private: bool = field(default=True, metadata={"help": "Make the dataset private on HuggingFace Hub"})

    # Tags for the dataset
    tags: list[str] | None = field(default=None, metadata={"help": "Tags to add to the dataset on the Hub"})

    # Store as video (True) or images (False)
    use_videos: bool = field(default=True, metadata={"help": "Store camera data as video (True) or images (False)"})

    # Max episode duration in seconds (None = keyboard only)
    max_episode_seconds: float | None = field(
        default=None, metadata={"help": "Maximum episode duration in seconds (None for keyboard-only control)"}
    )

    # Number of episodes to record (None = unlimited, use keyboard to stop)
    num_episodes: int | None = field(
        default=None, metadata={"help": "Number of episodes to record (None for unlimited)"}
    )

    # Number of image writer processes
    num_image_writer_processes: int = field(
        default=0, metadata={"help": "Number of subprocesses for saving frames as PNG (0 = threads only)"}
    )

    # Number of image writer threads per camera
    num_image_writer_threads_per_camera: int = field(
        default=4, metadata={"help": "Number of threads per camera for writing frames"}
    )

    # Video encoding batch size
    video_encoding_batch_size: int = field(
        default=1, metadata={"help": "Number of episodes to accumulate before batch encoding videos (1 = immediate encoding)"}
    )

    # Reset time between episodes
    reset_time_s: float = field(
        default=60.0, metadata={"help": "Number of seconds for resetting the environment after each episode"}
    )

    # Resume recording to an existing dataset
    resume: bool = field(
        default=False, metadata={"help": "Resume recording to an existing dataset instead of creating a new one"}
    )

    def __post_init__(self):
        """Validate configuration after initialization."""
        if self.enabled and not self.repo_id:
            raise ValueError("repo_id must be provided when dataset recording is enabled")

        if self.max_episode_seconds is not None and self.max_episode_seconds <= 0:
            raise ValueError(f"max_episode_seconds must be positive, got {self.max_episode_seconds}")

        if self.num_episodes is not None and self.num_episodes <= 0:
            raise ValueError(f"num_episodes must be positive, got {self.num_episodes}")

        if self.root is not None:
            self.root = Path(self.root)


@dataclass
class RobotClientConfig:
    """Configuration for RobotClient.

    This class defines all configurable parameters for the RobotClient,
    including network connection, policy settings, and control behavior.
    """

    # Policy configuration
    policy_type: str = field(metadata={"help": "Type of policy to use"})
    pretrained_name_or_path: str = field(metadata={"help": "Pretrained model name or path"})

    # Robot configuration (for CLI usage - robot instance will be created from this)
    robot: RobotConfig = field(metadata={"help": "Robot configuration"})

    # Policies typically output K actions at max, but we can use less to avoid wasting bandwidth (as actions
    # would be aggregated on the client side anyway, depending on the value of `chunk_size_threshold`)
    actions_per_chunk: int = field(metadata={"help": "Number of actions per chunk"})

    # Task instruction for the robot to execute (e.g., 'fold my tshirt')
    task: str = field(default="", metadata={"help": "Task instruction for the robot to execute"})

    # Network configuration
    server_address: str = field(default="localhost:8080", metadata={"help": "Server address to connect to"})

    # Device configuration
    policy_device: str = field(default="cpu", metadata={"help": "Device for policy inference"})

    # Control behavior configuration
    chunk_size_threshold: float = field(default=0.5, metadata={"help": "Threshold for chunk size control"})
    fps: int = field(default=DEFAULT_FPS, metadata={"help": "Frames per second"})

    # Aggregate function configuration (CLI-compatible)
    aggregate_fn_name: str = field(
        default="weighted_average",
        metadata={"help": f"Name of aggregate function to use. Options: {list(AGGREGATE_FUNCTIONS.keys())}"},
    )

    # Debug configuration
    debug_visualize_queue_size: bool = field(
        default=False, metadata={"help": "Visualize the action queue size"}
    )

    # Dataset recording configuration
    dataset: DatasetRecordingConfig = field(
        default_factory=DatasetRecordingConfig,
        metadata={"help": "Configuration for dataset recording during evaluation"},
    )

    # Optional teleoperator for reset periods ONLY (completely disabled during policy inference)
    teleop: TeleoperatorConfig | None = field(
        default=None,
        metadata={"help": "Teleoperator for reset periods only. Disabled during policy inference."},
    )

    @property
    def environment_dt(self) -> float:
        """Environment time step, in seconds"""
        return 1 / self.fps

    def __post_init__(self):
        """Validate configuration after initialization."""
        if not self.server_address:
            raise ValueError("server_address cannot be empty")

        if not self.policy_type:
            raise ValueError("policy_type cannot be empty")

        if not self.pretrained_name_or_path:
            raise ValueError("pretrained_name_or_path cannot be empty")

        if not self.policy_device:
            raise ValueError("policy_device cannot be empty")

        if self.chunk_size_threshold < 0 or self.chunk_size_threshold > 1:
            raise ValueError(f"chunk_size_threshold must be between 0 and 1, got {self.chunk_size_threshold}")

        if self.fps <= 0:
            raise ValueError(f"fps must be positive, got {self.fps}")

        if self.actions_per_chunk <= 0:
            raise ValueError(f"actions_per_chunk must be positive, got {self.actions_per_chunk}")

        self.aggregate_fn = get_aggregate_function(self.aggregate_fn_name)

    @classmethod
    def from_dict(cls, config_dict: dict) -> "RobotClientConfig":
        """Create a RobotClientConfig from a dictionary."""
        return cls(**config_dict)

    def to_dict(self) -> dict:
        """Convert the configuration to a dictionary."""
        return {
            "server_address": self.server_address,
            "policy_type": self.policy_type,
            "pretrained_name_or_path": self.pretrained_name_or_path,
            "policy_device": self.policy_device,
            "chunk_size_threshold": self.chunk_size_threshold,
            "fps": self.fps,
            "actions_per_chunk": self.actions_per_chunk,
            "task": self.task,
            "debug_visualize_queue_size": self.debug_visualize_queue_size,
            "aggregate_fn_name": self.aggregate_fn_name,
            "dataset": {
                "enabled": self.dataset.enabled,
                "repo_id": self.dataset.repo_id,
                "root": str(self.dataset.root) if self.dataset.root else None,
                "push_to_hub": self.dataset.push_to_hub,
                "use_videos": self.dataset.use_videos,
                "max_episode_seconds": self.dataset.max_episode_seconds,
            },
        }
